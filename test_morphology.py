#!/usr/bin/env python3
"""
Test morphological transformations for all 7 verb types.
Based on the comprehensive Arabic morphology guide.
"""

import sys
import requests
import json

BASE_URL = "http://localhost:8000"

# Test cases: (root, verb_type, [(scheme_id, expected_output_pattern)])
TEST_CASES = [
    # 1. ÿµÿ≠Ÿäÿ≠ ÿ≥ÿßŸÑŸÖ (Regular) - ŸÉÿ™ÿ®
    {
        "root": "ŸÉÿ™ÿ®",
        "verb_type": "ÿµÿ≠Ÿäÿ≠ ÿ≥ÿßŸÑŸÖ",
        "tests": [
            ("ŸÅÿπŸÑ", "ŸÉÿ™ÿ®"),        # ŸÖÿßÿ∂Ÿä: ŸÅŸéÿπŸéŸÑŸé
            ("ŸäŸÅÿπŸÑ", "ŸäŸÉÿ™ÿ®"),      # ŸÖÿ∂ÿßÿ±ÿπ: ŸäŸéŸÅŸíÿπŸéŸÑŸè
            ("ÿ£ŸÖÿ±", "ÿßŸÉÿ™ÿ®"),       # ÿ£ŸÖÿ±: ÿßŸêŸÅŸíÿπŸéŸÑŸí
            ("ŸÅÿßÿπŸÑ", "ŸÉÿßÿ™ÿ®"),      # ÿßÿ≥ŸÖ ÿßŸÑŸÅÿßÿπŸÑ: ŸÅŸéÿßÿπŸêŸÑ
            ("ŸÖŸÅÿπŸàŸÑ", "ŸÖŸÉÿ™Ÿàÿ®"),    # ÿßÿ≥ŸÖ ÿßŸÑŸÖŸÅÿπŸàŸÑ: ŸÖŸéŸÅŸíÿπŸèŸàŸÑ
        ]
    },
    
    # 2. ŸÖÿ´ÿßŸÑ ŸàÿßŸàŸä (Weak at start: Ÿà) - Ÿàÿ¨ÿØ
    {
        "root": "Ÿàÿ¨ÿØ",
        "verb_type": "ŸÖÿ´ÿßŸÑ ŸàÿßŸàŸä",
        "tests": [
            ("ŸÅÿπŸÑ", "Ÿàÿ¨ÿØ"),        # ŸÖÿßÿ∂Ÿä: ŸàŸéŸÅŸéÿπŸéŸÑŸé (keeps Ÿà)
            ("ŸäŸÅÿπŸÑ", "Ÿäÿ¨ÿØ"),       # ŸÖÿ∂ÿßÿ±ÿπ: ŸäŸéŸÅŸíÿπŸêŸÑŸè (drops Ÿà)
            ("ÿ£ŸÖÿ±", "ÿ¨ÿØ"),         # ÿ£ŸÖÿ±: ŸÅŸêÿπŸíŸÑ (drops Ÿà)
            ("ŸÅÿßÿπŸÑ", "Ÿàÿßÿ¨ÿØ"),      # ÿßÿ≥ŸÖ ÿßŸÑŸÅÿßÿπŸÑ: ŸàŸéÿßŸÅŸêÿπŸêŸÑ (keeps Ÿà)
            ("ŸÖŸÅÿπŸàŸÑ", "ŸÖŸàÿ¨ŸàÿØ"),    # ÿßÿ≥ŸÖ ÿßŸÑŸÖŸÅÿπŸàŸÑ: ŸÖŸéŸàŸíŸÅŸèŸàÿπ (keeps ŸÖ + Ÿà)
        ]
    },
    
    # 3. ÿ£ÿ¨ŸàŸÅ ŸàÿßŸàŸä (Weak at middle: Ÿà) - ŸÇÿßŸÑ
    {
        "root": "ŸÇÿßŸÑ",
        "verb_type": "ÿ£ÿ¨ŸàŸÅ ŸàÿßŸàŸä",
        "tests": [
            ("ŸÅÿπŸÑ", "ŸÇÿßŸÑ"),       # ŸÖÿßÿ∂Ÿä: ŸÅŸéÿßŸÑŸé (ÿπ=Ÿà becomes ÿß)
            ("ŸäŸÅÿπŸÑ", "ŸäŸÇŸàŸÑ"),     # ŸÖÿ∂ÿßÿ±ÿπ: ŸäŸéŸÅŸèŸàŸÑŸè (weak Ÿà returns)
            ("ÿ£ŸÖÿ±", "ŸÇŸÑ"),        # ÿ£ŸÖÿ±: ŸÅŸèŸÑŸí (short form)
            ("ŸÅÿßÿπŸÑ", "ŸÇÿßÿ¶ŸÑ"),     # ÿßÿ≥ŸÖ ÿßŸÑŸÅÿßÿπŸÑ: ŸÅŸéÿßÿ¶ŸêŸÑ (ÿß+ÿß‚Üíÿßÿ¶)
            ("ŸÖŸÅÿπŸàŸÑ", "ŸÖŸÇŸàŸÑ"),    # ÿßÿ≥ŸÖ ÿßŸÑŸÖŸÅÿπŸàŸÑ: ŸÖŸéŸÅŸèŸàŸÑ (keeps Ÿà from pattern)
        ]
    },
    
    # 4. ŸÜÿßŸÇÿµ Ÿäÿßÿ¶Ÿä (Weak at end: Ÿä/Ÿâ) - ÿ®ŸÇŸä
    {
        "root": "ÿ®ŸÇŸä",
        "verb_type": "ŸÜÿßŸÇÿµ Ÿäÿßÿ¶Ÿä",
        "tests": [
            ("ŸÅÿπŸÑ", "ÿ®ŸÇŸä"),       # ŸÖÿßÿ∂Ÿä: ŸÅŸéÿπŸéÿß (original ÿß)
            ("ŸäŸÅÿπŸÑ", "Ÿäÿ®ŸÇŸâ"),     # ŸÖÿ∂ÿßÿ±ÿπ: ŸäŸéŸÅŸíÿπŸèŸà (Ÿä‚ÜíŸâ, Ÿà from pattern)
            ("ÿ£ŸÖÿ±", "ÿßÿ®ŸÇ"),       # ÿ£ŸÖÿ±: ÿßŸêŸÅŸíÿπŸè (drops final Ÿä)
            ("ŸÅÿßÿπŸÑ", "ÿ®ÿßŸÇ"),      # ÿßÿ≥ŸÖ ÿßŸÑŸÅÿßÿπŸÑ: ŸÅŸéÿßÿπŸç (drops Ÿä)
            ("ŸÖŸÅÿπŸàŸÑ", "ŸÖÿ®ŸÇŸàŸä"),   # ÿßÿ≥ŸÖ ÿßŸÑŸÖŸÅÿπŸàŸÑ: ŸÖŸéŸÅŸíÿπŸèŸàŸë (keeps Ÿà+Ÿä)
        ]
    },
    
    # 5. ŸÜÿßŸÇÿµ ŸàÿßŸàŸä (Weak at end: ÿß/Ÿà) - ÿØÿπÿß
    {
        "root": "ÿØÿπÿß",
        "verb_type": "ŸÜÿßŸÇÿµ ŸàÿßŸàŸä",
        "tests": [
            ("ŸÅÿπŸÑ", "ÿØÿπÿß"),       # ŸÖÿßÿ∂Ÿä: ŸÅŸéÿπŸéÿß
            ("ŸäŸÅÿπŸÑ", "ŸäÿØÿπŸà"),     # ŸÖÿ∂ÿßÿ±ÿπ: ŸäŸéŸÅŸíÿπŸèŸà (ÿß‚ÜíŸà)
            ("ÿ£ŸÖÿ±", "ÿßÿØÿπ"),       # ÿ£ŸÖÿ±: ÿßŸêŸÅŸíÿπŸè (drops ÿß)
            ("ŸÅÿßÿπŸÑ", "ÿØÿßÿπ"),      # ÿßÿ≥ŸÖ ÿßŸÑŸÅÿßÿπŸÑ: ŸÅŸéÿßÿπŸç (drops ÿß)
            ("ŸÖŸÅÿπŸàŸÑ", "ŸÖÿØÿπŸà"),    # ÿßÿ≥ŸÖ ÿßŸÑŸÖŸÅÿπŸàŸÑ: ŸÖŸéŸÅŸíÿπŸèŸàŸÑ (ÿß‚ÜíŸà)
        ]
    },
    
    # 6. ŸÑŸÅŸäŸÅ ŸÖŸÅÿ±ŸàŸÇ (Weak at start AND end) - ŸàŸÇŸâ
    {
        "root": "ŸàŸÇŸâ",
        "verb_type": "ŸÑŸÅŸäŸÅ ŸÖŸÅÿ±ŸàŸÇ",
        "tests": [
            ("ŸÅÿπŸÑ", "ŸàŸÇŸâ"),       # ŸÖÿßÿ∂Ÿä: ŸàŸéŸÅŸéÿπŸéÿß
            ("ŸäŸÅÿπŸÑ", "ŸäŸÇŸä"),      # ŸÖÿ∂ÿßÿ±ÿπ: ŸäŸéŸÅŸíÿπŸêŸä (drops initial Ÿà)
            ("ÿ£ŸÖÿ±", "ŸÇŸä"),        # ÿ£ŸÖÿ±: ŸÅŸê (very short)
            ("ŸÅÿßÿπŸÑ", "ŸàÿßŸÇ"),      # ÿßÿ≥ŸÖ ÿßŸÑŸÅÿßÿπŸÑ: ŸàŸéÿßŸÅŸç (drops final Ÿä)
            ("ŸÖŸÅÿπŸàŸÑ", "ŸÖŸàŸÇŸä"),    # ÿßÿ≥ŸÖ ÿßŸÑŸÖŸÅÿπŸàŸÑ: ŸÖŸéŸàŸíŸÅŸêŸäŸë
        ]
    },
    
    # 7. ŸÑŸÅŸäŸÅ ŸÖŸÇÿ±ŸàŸÜ (Weak at middle AND end) - ÿ∑ŸàŸâ
    {
        "root": "ÿ∑ŸàŸâ",
        "verb_type": "ŸÑŸÅŸäŸÅ ŸÖŸÇÿ±ŸàŸÜ",
        "tests": [
            ("ŸÅÿπŸÑ", "ÿ∑ŸàŸâ"),       # ŸÖÿßÿ∂Ÿä: ŸÅŸéŸàŸéŸâ
            ("ŸäŸÅÿπŸÑ", "Ÿäÿ∑ŸàŸä"),     # ŸÖÿ∂ÿßÿ±ÿπ: ŸäŸéŸÅŸíŸàŸêŸä
            ("ÿ£ŸÖÿ±", "ÿßÿ∑Ÿà"),       # ÿ£ŸÖÿ±: ÿßŸêŸÅŸíŸàŸê
            ("ŸÅÿßÿπŸÑ", "ÿ∑ÿßŸà"),      # ÿßÿ≥ŸÖ ÿßŸÑŸÅÿßÿπŸÑ: ŸÅŸéÿßŸàŸç (drops final Ÿä)
            ("ŸÖŸÅÿπŸàŸÑ", "ŸÖÿ∑ŸàŸä"),    # ÿßÿ≥ŸÖ ÿßŸÑŸÖŸÅÿπŸàŸÑ: ŸÖŸéŸÅŸíŸàŸêŸäŸë
        ]
    },
    
    # Bonus: ŸÖÿ∂ÿßÿπŸÅ (Doubled) - ŸÖÿØÿØ
    {
        "root": "ŸÖÿØÿØ",
        "verb_type": "ŸÖÿ∂ÿßÿπŸÅ",
        "tests": [
            ("ŸÅÿπŸÑ", "ŸÖÿØÿØ"),       # ŸÖÿßÿ∂Ÿä: ŸÅŸéÿπŸéŸë
            ("ŸäŸÅÿπŸÑ", "ŸäŸÖÿØÿØ"),     # ŸÖÿ∂ÿßÿ±ÿπ: ŸäŸéŸÅŸéÿπŸèŸë
            ("ÿ£ŸÖÿ±", "ÿßŸÖÿØÿØ"),      # ÿ£ŸÖÿ±: ŸÅŸéÿπŸéŸë
            ("ŸÅÿßÿπŸÑ", "ŸÖÿßÿØÿØ"),     # ÿßÿ≥ŸÖ ÿßŸÑŸÅÿßÿπŸÑ: ŸÅŸéÿßÿπŸë
            ("ŸÖŸÅÿπŸàŸÑ", "ŸÖŸÖÿØŸàÿØ"),   # ÿßÿ≥ŸÖ ÿßŸÑŸÖŸÅÿπŸàŸÑ: ŸÖŸéŸÅŸíÿπŸèŸàŸÑ
        ]
    },
    
    # ŸÖŸáŸÖŸàÿ≤ ÿßŸÑŸÅÿßÿ° (Hamza at start) - ÿ£ŸÉŸÑ
    {
        "root": "ÿ£ŸÉŸÑ",
        "verb_type": "ŸÖŸáŸÖŸàÿ≤ ÿßŸÑŸÅÿßÿ°",
        "tests": [
            ("ŸÅÿπŸÑ", "ÿ£ŸÉŸÑ"),       # ŸÖÿßÿ∂Ÿä: ŸÅŸéÿπŸéŸÑŸé
            ("ŸäŸÅÿπŸÑ", "Ÿäÿ£ŸÉŸÑ"),     # ŸÖÿ∂ÿßÿ±ÿπ: ŸäŸéŸÅŸíÿπŸéŸÑŸè
            ("ÿ£ŸÖÿ±", "ÿßŸÉŸÑ"),       # ÿ£ŸÖÿ±: ÿßŸêŸÅŸíÿπŸéŸÑŸí (drops hamza)
            ("ŸÅÿßÿπŸÑ", "ÿ¢ŸÉŸÑ"),      # ÿßÿ≥ŸÖ ÿßŸÑŸÅÿßÿπŸÑ: ŸÅŸéÿßÿπŸêŸÑ (ÿß+ÿß‚Üíÿ¢)
            ("ŸÖŸÅÿπŸàŸÑ", "ŸÖÿßŸÉŸàŸÑ"),   # ÿßÿ≥ŸÖ ÿßŸÑŸÖŸÅÿπŸàŸÑ: ŸÖŸéŸÅŸíÿπŸèŸàŸÑ
        ]
    },
]

def normalize_for_comparison(text: str) -> str:
    """Normalize text for comparison (remove diacritics, standardize alefs)."""
    import re
    text = re.sub(r'[\u064B-\u0652]', '', text)  # Remove harakat
    text = text.replace('ÿ¢', 'ÿß')
    text = text.replace('ÿ£', 'ÿß')
    text = text.replace('ÿ•', 'ÿß')
    text = text.replace('Ÿâ', 'Ÿä')
    return text.strip()

def test_generation():
    """Test word generation for all verb types."""
    print("\n" + "="*80)
    print("üß™ TESTING MORPHOLOGICAL TRANSFORMATIONS")
    print("="*80)
    
    passed = 0
    failed = 0
    
    for test_group in TEST_CASES:
        root = test_group["root"]
        verb_type = test_group["verb_type"]
        
        print(f"\nüìå Testing: {root} ({verb_type})")
        print("-" * 60)
        
        for scheme_id, expected_pattern in test_group["tests"]:
            try:
                # Call API to generate
                response = requests.post(
                    f"{BASE_URL}/api/generate",
                    params={"root": root, "scheme_id": scheme_id}
                )
                
                if response.status_code != 200:
                    print(f"  ‚ùå {scheme_id:10} | API Error: {response.status_code}")
                    failed += 1
                    continue
                
                result = response.json()
                generated_word = result.get("word", "")
                
                # Normalize both for comparison
                gen_norm = normalize_for_comparison(generated_word)
                exp_norm = normalize_for_comparison(expected_pattern)
                
                # Check if match
                if gen_norm == exp_norm:
                    print(f"  ‚úÖ {scheme_id:10} | {generated_word:15} (expected pattern: {expected_pattern})")
                    passed += 1
                else:
                    print(f"  ‚ùå {scheme_id:10} | Got: {generated_word:15} Expected: {expected_pattern}")
                    failed += 1
                    
            except Exception as e:
                print(f"  ‚ùå {scheme_id:10} | Exception: {str(e)}")
                failed += 1
    
    print("\n" + "="*80)
    print(f"üìä RESULTS: {passed} passed, {failed} failed out of {passed + failed}")
    print("="*80)
    
    return failed == 0

if __name__ == "__main__":
    success = test_generation()
    sys.exit(0 if success else 1)
